{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hacker News Interaction Analysis: What Types of Posts Receive the Most Comments and When?\n",
    "\n",
    "In this analysis, we will be focusing on a dataset from Kaggle containing 12-months of post information from the social technology forum 'Hacker News' between September 2015 and 2016. We are interested in using it to understand more about user interaction on the site. Specifically to answer the following:\n",
    "1. Of the categorized types of posts a user can generate, between 'Ask HN' posts and 'Show HN' posts, which receives more interaction?\n",
    "2. Do posts created at a certain time receive more comments on average?\n",
    "\n",
    "Let us examine the breakdown of the data we will be analyzing. Below are the column headers and their description:\n",
    "    \n",
    "    id: The unique identifier from Hacker News for the post\n",
    "    title: The title of the post\n",
    "    url: The URL that the posts links to, if the post has a URL\n",
    "    num_points: The number of points the post acquired, calculated as the total number of upvotes minus the total number of downvotes\n",
    "    num_comments: The number of comments that were made on the post\n",
    "    author: The username of the person who submitted the post\n",
    "    created_at: The date and time at which the post was submitted\n",
    "\n",
    "We will import the dataset from Kaggle (https://www.kaggle.com/hacker-news/hacker-news-posts) locally and display the first five rows below to give us a snapshot of what the data looks like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['id', 'title', 'url', 'num_points', 'num_comments', 'author', 'created_at']\n",
      "['12579008', 'You have two days to comment if you want stem cells to be classified as your own', 'http://www.regulations.gov/document?D=FDA-2015-D-3719-0018', '1', '0', 'altstar', '9/26/2016 3:26']\n",
      "['12579005', 'SQLAR  the SQLite Archiver', 'https://www.sqlite.org/sqlar/doc/trunk/README.md', '1', '0', 'blacksqr', '9/26/2016 3:24']\n",
      "['12578997', 'What if we just printed a flatscreen television on the side of our boxes?', 'https://medium.com/vanmoof/our-secrets-out-f21c1f03fdc8#.ietxmez43', '1', '0', 'pavel_lishin', '9/26/2016 3:19']\n",
      "['12578989', 'algorithmic music', 'http://cacm.acm.org/magazines/2011/7/109891-algorithmic-composition/fulltext', '1', '0', 'poindontcare', '9/26/2016 3:16']\n"
     ]
    }
   ],
   "source": [
    "from csv import reader\n",
    "open_file = open(r'C:\\Users\\bbeckenb\\OneDrive\\Documents\\Local Datasets\\Hacker_News_Post_Data.csv', encoding=\"utf8\")\n",
    "read_file = reader(open_file)\n",
    "hn = list(read_file)\n",
    "\n",
    "for row in hn[:5]:\n",
    "    print(row)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Cleaning\n",
    "Now, we will need to clean the data. I will go through a process to remove:\n",
    "1. Erroneous data\n",
    "2. Duplicate data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Removing Erroneous data:\n",
    "To qualify as erroneous, the data must be incomplete. To check this, we will write a script to compare the length of the column header to the length of all rows that follow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The length of the original Hacker News dataset is 293119\n",
      "The erroneous data script found 0 erroneous rows\n"
     ]
    }
   ],
   "source": [
    "print(f\"The length of the original Hacker News dataset is {len(hn[1:])}\")\n",
    "hn_erroneous = []\n",
    "\n",
    "for row in hn[1:]:\n",
    "    header_length = len(hn[0])\n",
    "    row_length = len(row)\n",
    "    if row_length != header_length:\n",
    "        hn_erroneous.append(row)\n",
    "\n",
    "print(f\"The erroneous data script found {len(hn_erroneous)} erroneous rows\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next step in our data cleansing process is to check for duplicates. To accomplish this, we will store a list of the unique ID numbers stored in column 0 and run this new list through a filter. The filter will run the ID list through two empty lists (clean and duplicate) using a for loop. We will append rows to the clean list after checking if they are already in the list. If they are already in the list, the logic will append them to the duplicates list instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The duplicate data script found 0 duplicate rows, there are 20000 clean rows\n"
     ]
    }
   ],
   "source": [
    "#This filtering takes much too long on my local machine, I will find a solution for this, but when I ran it all the way through there were 0 duplicate rows.\n",
    "unique_id = []\n",
    "\n",
    "for row in hn[1:]:\n",
    "    u_id = row[0]\n",
    "    unique_id.append(u_id)\n",
    "\n",
    "\n",
    "clean_list = []\n",
    "duplicate_list = []\n",
    "for element in unique_id[:20000]:\n",
    "    if element in clean_list:\n",
    "        duplicate_list.append(element)\n",
    "    else:\n",
    "        clean_list.append(element)\n",
    "        \n",
    "print(f\"The duplicate data script found {len(duplicate_list)} duplicate rows, there are {len(clean_list)} clean rows\")        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have determined there are no erroneous rows of data and no duplicate rows, we can move forward with our questions at hand.\n",
    "\n",
    "We are looking for information that guides us on the following queries:\n",
    "1. Of the categorized types of posts a user can generate, between 'Ask HN' posts and 'Show HN' posts, which receives more interaction?\n",
    "2. Do posts created at a certain time receive more comments on average?\n",
    "\n",
    "# Between 'Ask HN' posts and 'Show HN' posts, which receives more interaction?\n",
    "We are looking for an average interaction number or score from two categories of posts, 'Ask Hacker News' and 'Show Hacker News'. To help find this, we will need to separate the two different categories of data into separate lists then focus in on the num_comments and num_points information. To refresh, here are the definitions of these two columns of data:\n",
    "    \n",
    "    num_points: The number of points the post acquired, calculated as the total number of upvotes minus the total number of downvotes\n",
    "    num_comments: The number of comments that were made on the post   \n",
    "    \n",
    "To separate the 'Ask Hacker News' and 'Show Hacker News' data points, we will write a script to put the two in different lists. 'Ask Hacker News' rows have \"Ask HN:\" in the beginning of their title. In the script, we will check if \"Ask HN:\" is in the title for each row. If it is, we will put it in an Ask Hacker News list, if it contains \"Show HN:\", we will put the row in a Show Hacker News list, anything else, we will put in a miscellaneous list named 'other_hn_list'. We will check the length total of our three new lists against the length of the original list to see the percentage breakdown."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 9110 'Ask HN' posts and 10140 'Show HN' posts in our dataset, giving us 3.11% Ask posts and 3.46% Show posts\n"
     ]
    }
   ],
   "source": [
    "ask_hn_list = []\n",
    "show_hn_list = []\n",
    "other_hn_list = []\n",
    "\n",
    "for row in hn[1:]:\n",
    "    title = row[1]\n",
    "    if \"Ask HN:\" in title:\n",
    "        ask_hn_list.append(row)\n",
    "    elif \"Show HN:\" in title:\n",
    "        show_hn_list.append(row)\n",
    "    else:\n",
    "        other_hn_list.append(row)\n",
    "\n",
    "ask_percentage = len(ask_hn_list) / len(hn) * 100\n",
    "show_percentage = len(show_hn_list) / len(hn) * 100\n",
    "print(f\"There are {len(ask_hn_list)} 'Ask HN' posts and {len(show_hn_list)} 'Show HN' posts in our dataset, giving us {round(ask_percentage, 2)}% Ask posts and {round(show_percentage, 2)}% Show posts\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have separated our data, we can see we have a similar amount of 'Show HN' posts (3.46%) when compared to 'Ask Posts' (3.11%). We can use the interaction information provided in the rows to establish an average interaction score to help us understand whether Asking or Showing on Hacker News typically receives more interactions.\n",
    "\n",
    "We will utilize num_points and num_comments where num_points is the total number of upvotes minus the total number of downvotes and num_comments is the number of comments made on the post to help us. We will group average points, average comments, and average interaction (comments plus points) separately for each list to give an initial impression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The average points for the 'Ask' List is 11.33, the average comments for the 'Ask' List is 10.41, average total interaction (comments plus points) is 21.74\n",
      "The average points for the 'Show' List is 14.87, the average comments for the 'Show' List is 4.89, average total interaction (comments plus points) is 19.76\n"
     ]
    }
   ],
   "source": [
    "tot_points_shn = 0\n",
    "tot_points_ahn = 0\n",
    "tot_comments_shn = 0 \n",
    "tot_comments_ahn = 0\n",
    "\n",
    "for row in ask_hn_list:\n",
    "    points = int(row[3])\n",
    "    comments = int(row[4])\n",
    "    tot_points_ahn += points\n",
    "    tot_comments_ahn += comments\n",
    "    \n",
    "for row in show_hn_list:\n",
    "    points = int(row[3])\n",
    "    comments = int(row[4])\n",
    "    tot_points_shn += points\n",
    "    tot_comments_shn += comments\n",
    "    \n",
    "avg_points_ahn = round(tot_points_ahn / len(ask_hn_list), 2)\n",
    "avg_points_shn = round(tot_points_shn / len(show_hn_list), 2)\n",
    "avg_comments_ahn = round(tot_comments_ahn / len(ask_hn_list), 2)\n",
    "avg_comments_shn = round(tot_comments_shn / len(show_hn_list), 2)\n",
    "avg_points_and_comments_ahn = round((tot_points_ahn + tot_comments_ahn) / len(ask_hn_list), 2)                                 \n",
    "avg_points_and_comments_shn = round((tot_points_shn + tot_comments_shn) / len(show_hn_list), 2)\n",
    "\n",
    "print(f\"The average points for the 'Ask' List is {avg_points_ahn}, the average comments for the 'Ask' List is {avg_comments_ahn}, average total interaction (comments plus points) is {avg_points_and_comments_ahn}\")\n",
    "print(f\"The average points for the 'Show' List is {avg_points_shn}, the average comments for the 'Show' List is {avg_comments_shn}, average total interaction (comments plus points) is {avg_points_and_comments_shn}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see from an initial pass, average points for \"Show\" posts are 23.81% higher when compared to \"Ask\" posts. Meanwhile, average comments for the \"Ask\" posts are 53.03% higher compared to \"Show\" Posts. This makes sense when thinking about the nature of the different categories. When you are \"Showing\" some technology you have found in the forum, it is likely something exciting or cutting edge which may appeal to some stripe of the general Hacker News user base. Meanwhile, I would expect comments to be higher for an \"Ask\" post compared to a \"Show\" post due to the fact that the action of asking is a request for a response. The average of total interaction (likes plus comments given equal weight) is different by 9.1% with 'Ask' posts having the edge.\n",
    "\n",
    "Let's breakdown the data more to see what other insights may be gleaned. Our next action will be to create frequency distribution tables to display where a majority of the comment and upvote magnitudes are. I will define a function to generate these frequency distribution tables, then call it for the comment and upvote columns of the 'Ask' and 'Show' lists. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Comments in Ask List : (Frequency) : Percentage of 9110 Total Data Points\n",
      "x = 0 : ( 1941 ) : 21.31 % \n",
      "\n",
      "0 < x <= 1 : ( 564 ) : 6.19 % \n",
      "\n",
      "1 < x <= 2 : ( 853 ) : 9.36 % \n",
      "\n",
      "2 < x <= 3 : ( 2548 ) : 27.97 % \n",
      "\n",
      "3 < x <= 4 : ( 76 ) : 0.83 % \n",
      "\n",
      "4 < x : ( 3128 ) : 34.34 % \n",
      "\n",
      "Upvotes in Ask List : (Frequency) : Percentage of 9110 Total Data Points\n",
      "x = 0 : ( 0 ) : 0.0 % \n",
      "\n",
      "0 < x <= 1 : ( 3878 ) : 42.57 % \n",
      "\n",
      "1 < x <= 2 : ( 1031 ) : 11.32 % \n",
      "\n",
      "2 < x <= 3 : ( 163 ) : 1.79 % \n",
      "\n",
      "3 < x <= 4 : ( 26 ) : 0.29 % \n",
      "\n",
      "4 < x : ( 4012 ) : 44.04 % \n",
      "\n",
      "Comments in Show List : (Frequency) : Percentage of 10140 Total Data Points\n",
      "x = 0 : ( 8971 ) : 88.47 % \n",
      "\n",
      "0 < x <= 1 : ( 942 ) : 9.29 % \n",
      "\n",
      "1 < x <= 2 : ( 33 ) : 0.33 % \n",
      "\n",
      "2 < x <= 3 : ( 54 ) : 0.53 % \n",
      "\n",
      "3 < x <= 4 : ( 4 ) : 0.04 % \n",
      "\n",
      "4 < x : ( 136 ) : 1.34 % \n",
      "\n",
      "Upvotes in Show List : (Frequency) : Percentage of 10140 Total Data Points\n",
      "x = 0 : ( 0 ) : 0.0 % \n",
      "\n",
      "0 < x <= 1 : ( 6734 ) : 66.41 % \n",
      "\n",
      "1 < x <= 2 : ( 1993 ) : 19.65 % \n",
      "\n",
      "2 < x <= 3 : ( 859 ) : 8.47 % \n",
      "\n",
      "3 < x <= 4 : ( 68 ) : 0.67 % \n",
      "\n",
      "4 < x : ( 486 ) : 4.79 % \n",
      "\n"
     ]
    }
   ],
   "source": [
    "def frequency_distribution(data_set, column_num, std_dev, context, int_convert = True):\n",
    "    column_list = []\n",
    "    for row in data_set:\n",
    "        if int_convert == True:\n",
    "            element = int(row[column_num])\n",
    "        else:\n",
    "            element = row[column_num]     \n",
    "        column_list.append(element)\n",
    "    freq_dist_dictionary = {}    \n",
    "    key = \"x = 0\"\n",
    "    freq_dist_dictionary[key] = 0\n",
    "    key_0 = f\"0 < x <= {std_dev}\"\n",
    "    freq_dist_dictionary[key_0] = 0\n",
    "    key_1 = f\"{std_dev} < x <= {std_dev * 2}\"\n",
    "    freq_dist_dictionary[key_1] = 0\n",
    "    key_2 = f\"{std_dev * 2} < x <= {std_dev * 3}\"\n",
    "    freq_dist_dictionary[key_2] = 0\n",
    "    key_3 = f\"{std_dev * 3} < x <= {std_dev * 4}\"\n",
    "    freq_dist_dictionary[key_3] = 0\n",
    "    key_4 = f\"{std_dev * 4} < x\"\n",
    "    freq_dist_dictionary[key_4] = 0\n",
    "    \n",
    "    for row in column_list:\n",
    "        if column_list[row] == 0:\n",
    "            freq_dist_dictionary[key] += 1\n",
    "        if column_list[row] <= std_dev and column_list[row] != 0:\n",
    "            freq_dist_dictionary[key_0] += 1\n",
    "        if column_list[row] > std_dev and column_list[row] <= (2 * std_dev):\n",
    "            freq_dist_dictionary[key_1] += 1\n",
    "        if column_list[row] > (2 * std_dev) and column_list[row] <= (3 * std_dev):\n",
    "            freq_dist_dictionary[key_2] += 1  \n",
    "        if column_list[row] > (3 * std_dev) and column_list[row] <= (4 * std_dev):\n",
    "            freq_dist_dictionary[key_3] += 1\n",
    "        if column_list[row] > (4 * std_dev):\n",
    "            freq_dist_dictionary[key_4] += 1\n",
    "    print(f\"{context} : (Frequency) : Percentage of {len(column_list)} Total Data Points\")\n",
    "    for row in freq_dist_dictionary:\n",
    "        print(row, ': (', freq_dist_dictionary[row], ') :', round(100 * (freq_dist_dictionary[row] / len(column_list)), 2), '% \\n')\n",
    "\n",
    "frequency_distribution(ask_hn_list, 4, 1, 'Comments in Ask List')\n",
    "frequency_distribution(ask_hn_list, 3, 1, 'Upvotes in Ask List')  \n",
    "frequency_distribution(show_hn_list, 4, 1, 'Comments in Show List')\n",
    "frequency_distribution(show_hn_list, 3, 1, 'Upvotes in Show List') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data lists are similar in length with the length of 'Show' Post data being 10140, and the length of 'Ask' Post data being 9110. The frequency distribution tables above give us an interesting break down of the upvotes and comments in the 'Show' and 'Ask' lists. Using '1' as the standard deviation, some of the returned numbers give us a strong indication of interaction behavior. These were the main points that stood out:\n",
    "1. There must be a default 1 upvote given immediately to posts because both the Ask and Show posts have no 0 vote posts\n",
    "2. 97.76% of posts in the Show list received 1 comment or less, 88.47% received 0 comments, only 1.34% of 'Show' posts received 5 or more comments\n",
    "3. 78.69% of posts in the Ask list receive at least 1 comment, over one third (34.34%) received at least 5 comments\n",
    "4. 4.79% of Show posts received 5 or more upvotes whereas 44.04% of the Ask posts received 5 or more upvotes. The magnitudes of the posts receiving 5 or more upvotes for 'Ask' and 'Show' posts are 4012 and 486 respectively, making 'Show' posts with upvotes of 5 or more stack up to only 12% of the 'Ask' posts with 5 or more upvotes.\n",
    "\n",
    "Our first objective in this analysis was to determine if Ask HN or Show HN posts receive more comments and/ or points on average. Given the data from the initial pass to find average points and comments and the insights from the frequency distribution tables above, we can confidently say Ask HN posts are more likely to be interacted with by general users on Hacker News. With 'Show' Posts having 11.53% of their posts with at least 1 comment and 'Ask' Posts having 78.69% of posts with at least 1 comment, you would be 6.82 times more likely to receive a comment on an 'Ask' Post. With 'Show' Posts having 33.59% of their posts with at least 1 outside upvote and 'Ask' Posts having 57.43% of posts with at least 1 outside upvote, you would be 1.71 times more likely to receive an upvote from another user on an 'Ask' Post. As mentioned previously, having more comments on an 'Ask' post makes a lot of sense given asking a question incites a response and that is supported strongly by the data.\n",
    "\n",
    "Now we will move onto the 2nd objective of this analysis: Do posts created at a certain time receive more comments on average?\n",
    "\n",
    "First we will revisit the make up of our data by looking at a few rows to refamiliarize ourselves and make decisions on which columns will be useful to accomplish this objective. Due to the nature of a time related question, we are already aware that the timestamp will be important. I checked its class type below to see what next steps we need to take."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['12578908', 'Ask HN: What TLD do you use for local development?', '', '4', '7', 'Sevrene', '9/26/2016 2:53'] \n",
      "\n",
      "['12578522', 'Ask HN: How do you pass on your work when you die?', '', '6', '3', 'PascLeRasc', '9/26/2016 1:17'] \n",
      "\n",
      "['12577908', 'Ask HN: How a DNS problem can be limited to a geographic region?', '', '1', '0', 'kuon', '9/25/2016 22:57'] \n",
      "\n",
      "['12577870', 'Ask HN: Why join a fund when you can be an angel?', '', '1', '3', 'anthony_james', '9/25/2016 22:48'] \n",
      "\n",
      "['12577647', 'Ask HN: Someone uses stock trading as passive income?', '', '5', '2', '00taffe', '9/25/2016 21:50'] \n",
      "\n",
      "Time Stamp, 'created_at' column data is type <class 'str'>\n"
     ]
    }
   ],
   "source": [
    "for row in ask_hn_list[:5]:\n",
    "    print(row, '\\n')\n",
    "\n",
    "print(f\"Time Stamp, 'created_at' column data is type {type(ask_hn_list[1][6])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see above, these are 'Ask HN' posts, but all rows in the data have the same format:\n",
    "    \n",
    "    id: The unique identifier from Hacker News for the post\n",
    "    title: The title of the post\n",
    "    url: The URL that the posts links to, if the post has a URL\n",
    "    num_points: The number of points the post acquired, calculated as the total number of upvotes minus the total number of downvotes\n",
    "    num_comments: The number of comments that were made on the post\n",
    "    author: The username of the person who submitted the post\n",
    "    created_at: The date and time at which the post was submitted\n",
    "    \n",
    "# When do Different kinds of Posts Receive the Most Comments?\n",
    "Our second objective is asking us to find a correlation between post time and number of comments to find the optimal time to post to receive the most comments. With that in mind, we will be focusing on the 'num_comments' and 'created_at' columns to guide us. We will need to find an average comments per post creation number over time to answer our question.\n",
    "\n",
    "We will look at three different categories from the original HN dataset: Ask HN post data, Show HN post data, and the entire HN list to see how they compare.\n",
    "\n",
    "We can see from the last row in our sample above the time stamp of the final row is '9/25/2016 21:50' giving the 'created_at' column a format of 'MM/DD/YYYY HH/mm' with hours going from 0 to 23. As we can see above, the column data for 'created_at' is already '<class 'str'>', so we will be able to manipulate it as such for the time-being. We will first group the 'comment_num' data with the 'created_date' data for the three datasets mentioned above. To improve efficiency and reduce code length, I am storing the three lists in aligned containers (meaning they are in the same order in the different containers). I will run the containers through the processing scripts. We will then print the first five rows of our new lists to give us an idea of what the data inside looks like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, '9/26/2016 3:26']\n",
      "[0, '9/26/2016 3:24']\n",
      "[0, '9/26/2016 3:19']\n",
      "[0, '9/26/2016 3:16']\n",
      "[0, '9/26/2016 3:14']\n",
      "\n",
      "\n",
      "[7, '9/26/2016 2:53']\n",
      "[3, '9/26/2016 1:17']\n",
      "[0, '9/25/2016 22:57']\n",
      "[3, '9/25/2016 22:48']\n",
      "[2, '9/25/2016 21:50']\n",
      "\n",
      "\n",
      "[0, '9/26/2016 0:36']\n",
      "[0, '9/26/2016 0:01']\n",
      "[0, '9/25/2016 23:44']\n",
      "[0, '9/25/2016 23:17']\n",
      "[1, '9/25/2016 20:06']\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "hn_no_header = hn[1:]\n",
    "shn_comments_dates = []\n",
    "ahn_comments_dates = []\n",
    "hn_no_header_comments_dates = []\n",
    "raw_list_container = [hn_no_header, ask_hn_list, show_hn_list]\n",
    "comments_dates_list_container = [hn_no_header_comments_dates, ahn_comments_dates, shn_comments_dates]\n",
    "list_count = 0\n",
    "\n",
    "for data_set in comments_dates_list_container:    \n",
    "    for row in raw_list_container[list_count]:\n",
    "        comments = int(row[4])\n",
    "        date = row[6]\n",
    "        comments_and_dates = [comments, date]\n",
    "        comments_dates_list_container[list_count].append(comments_and_dates)\n",
    "    list_count += 1\n",
    "\n",
    "list_count = 0\n",
    "for data_set in comments_dates_list_container: \n",
    "    for row in comments_dates_list_container[list_count][:5]:\n",
    "        print(row)\n",
    "    print('\\n')\n",
    "    list_count += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This interpretation of finding the optimal time to post to receive most comments is centered around hours and minutes of any given day. There certainly could be further analysis on a specific day or time of year to post, but for now we will focus our scope on the hours and minutes. To do this, we will first want to isolate the hours and minutes portion of our date elements. We will utilize the 'split()' function (https://www.w3schools.com/python/ref_string_split.asp) to change the date into a list of two strings, the 0th element containing the month/day/year, the 1st containing the hours and minutes information we want. We will store this truncated time information along with the comment information in new lists tagged with '_hours_min' in a new container tagged with the same.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, '3:26']\n",
      "[0, '3:24']\n",
      "[0, '3:19']\n",
      "[0, '3:16']\n",
      "[0, '3:14']\n",
      "\n",
      "\n",
      "[7, '2:53']\n",
      "[3, '1:17']\n",
      "[0, '22:57']\n",
      "[3, '22:48']\n",
      "[2, '21:50']\n",
      "\n",
      "\n",
      "[0, '0:36']\n",
      "[0, '0:01']\n",
      "[0, '23:44']\n",
      "[0, '23:17']\n",
      "[1, '20:06']\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "shn_comments_hours_min = []\n",
    "ahn_comments_hours_min = []\n",
    "hn_no_header_comments_hours_min = []\n",
    "comments_hours_mins_container = [hn_no_header_comments_hours_min, ahn_comments_hours_min, shn_comments_hours_min]\n",
    "list_count = 0 \n",
    "\n",
    "for data_set in comments_dates_list_container:\n",
    "    for row in comments_dates_list_container[list_count]:\n",
    "        comment = row[0]\n",
    "        date = row[1]\n",
    "        date = date.split(' ', 1)\n",
    "        hours_and_min = date[1]\n",
    "        new_row = [comment, hours_and_min]\n",
    "        comments_hours_mins_container[list_count].append(new_row)\n",
    "    list_count += 1\n",
    "\n",
    "list_count = 0\n",
    "for data_set in comments_hours_mins_container:     \n",
    "    for row in comments_hours_mins_container[list_count][:5]:\n",
    "        print(row)\n",
    "    print('\\n')\n",
    "    list_count += 1\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have isolated the appropriate information in a single list, we are going to create a series of frequency distribution tables to narrow in on an appropriate range of time that contains the most comments. Let's start with AM versus PM times. \n",
    "\n",
    "We will import the datetime class as dt with all of its available methods in order to convert the timestamps from the string class to datetime objects to take advantage of pre-built functionality to analyze and manipulate time information.\n",
    "\n",
    "The script below will establish two empty dictionaries, one to track comment magnitude over the AM and PM time-ranges, one to track posts generated during the two time ranges. That way we will be able to create an average by dividing the amounts of comments by the amount of posts over the time range in question. \n",
    "\n",
    "We will run the three datasets (All HN, Ask HN, and Show Hn) through the script, resetting then reusing the dictionaries 'am_vs_pm_comment_freq' and 'am_vs_pm_post_creation_freq' each iteration of the primary loop. In each iteration of the secondary loop, we will set the integer of the magnitude of comments to a variable 'comment' then the string timestamp of hours and minutes to variable 'hours_and_min'. We covert the string timestamp to a datetime object and compare it to 'noon', a datetime object we created representing the time 12:00. The logic is set so that if the 'hours_and_min' time is above or below 'noon' it will be sorted to the respective key for the two dictionaries 'AM' or 'PM'. The appropriate key of the dictionary will be incremented one for post frequency and the comments by the magnitude stored in the 'comment' variable.\n",
    "\n",
    "We then print the name of the dataset that was processed, a header for the information we are about to output to add context to the numbers, and the output data for each row in the dictionary (only 2, for 'AM' and 'PM')."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For All HN\n",
      "AM/ PM : Avg Comments/ Posts Created : (Comments Generated) : Percentage of Total Comments\n",
      "AM : 6.61 : ( 646982 ) : 33.82 %\n",
      "\n",
      "PM : 6.48 : ( 1265779 ) : 66.18 %\n",
      "\n",
      "For Ask HN\n",
      "AM/ PM : Avg Comments/ Posts Created : (Comments Generated) : Percentage of Total Comments\n",
      "AM : 8.55 : ( 26484 ) : 27.93 %\n",
      "\n",
      "PM : 11.36 : ( 68339 ) : 72.07 %\n",
      "\n",
      "For Show HN\n",
      "AM/ PM : Avg Comments/ Posts Created : (Comments Generated) : Percentage of Total Comments\n",
      "AM : 4.94 : ( 15166 ) : 30.56 %\n",
      "\n",
      "PM : 4.87 : ( 34454 ) : 69.44 %\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import datetime as dt\n",
    "from datetime import *\n",
    "\n",
    "am_vs_pm_comment_freq = {}\n",
    "am_vs_pm_post_creation_freq = {}\n",
    "data_set_names = ['All HN', 'Ask HN', 'Show HN']\n",
    "list_count = 0\n",
    "\n",
    "for data_set in comments_hours_mins_container:\n",
    "    am_vs_pm_comment_freq['AM'] = 0\n",
    "    am_vs_pm_comment_freq['PM'] = 0\n",
    "    am_vs_pm_post_creation_freq['AM'] = 0\n",
    "    am_vs_pm_post_creation_freq['PM'] = 0\n",
    "    total_comments = 0\n",
    "\n",
    "    for row in comments_hours_mins_container[list_count]:\n",
    "        comment = row[0]\n",
    "        hours_and_min = row[1]\n",
    "        hours_and_min = dt.datetime.strptime(hours_and_min, \"%H:%M\")\n",
    "        noon = dt.datetime.strptime('12:00', \"%H:%M\")\n",
    "        if hours_and_min >= noon: \n",
    "            am_vs_pm_comment_freq['PM'] += comment\n",
    "            am_vs_pm_post_creation_freq['PM'] += 1\n",
    "        else:\n",
    "            am_vs_pm_comment_freq['AM'] += comment\n",
    "            am_vs_pm_post_creation_freq['AM'] += 1\n",
    "        total_comments += comment\n",
    "    print(f\"For {data_set_names[list_count]}\")\n",
    "    print('AM/ PM : Avg Comments/ Posts Created : (Comments Generated) : Percentage of Total Comments')\n",
    "    for row in am_vs_pm_comment_freq:\n",
    "        print(row, ':', round(am_vs_pm_comment_freq[row]/ am_vs_pm_post_creation_freq[row], 2), ': (', am_vs_pm_comment_freq[row], ') :', round((100 * am_vs_pm_comment_freq[row] / total_comments), 2), '%\\n')\n",
    "    list_count += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From our first sweep, we can see that the average comments per post is fairly close for the overall list and Show HN posts, the 'Ask HN' list is 24.74% higher in the afternoon. Looking at Comments generated and the percentage of total comments, we can see the general amount of activity is much higher in the afternoon. Overall list comments roughly double in the afternoon, Ask HN comments almost triple, Show HN comments over double. However, we still do not have enough resolution to answer our question. In this next sweep, we will have four rows in our frequency distribution table, each consisting of a 6 hour window: 0:00 - 5:59, 6:00 - 11:59, 12:00 - 17:59, 18:00 - 23:59.\n",
    "\n",
    "The process for this sweep is the same as the 'AM' vs 'PM' sweep, with two caveats: \n",
    "1. The magnitude of comments and the amount of posts created will be stored in a list in the same dictionary in the format \"[comments, posts]\". \n",
    "2. We are looking at additional time-windows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For All HN\n",
      "4-Quarters of Day : Average Comments per Post : (Magnitude of Comments) : Percentage of Total Comments\n",
      "0:00 - 5:59 : 6.68 : ( 301219 ) : 15.75 %\n",
      "\n",
      "6:00 - 11:59 : 6.55 : ( 345763 ) : 18.08 %\n",
      "\n",
      "12:00 - 17:59 : 6.79 : ( 721066 ) : 37.7 %\n",
      "\n",
      "18:00 - 23:59 : 6.12 : ( 544713 ) : 28.48 %\n",
      "\n",
      "For Ask HN\n",
      "4-Quarters of Day : Average Comments per Post : (Magnitude of Comments) : Percentage of Total Comments\n",
      "0:00 - 5:59 : 8.72 : ( 13669 ) : 14.42 %\n",
      "\n",
      "6:00 - 11:59 : 8.38 : ( 12815 ) : 13.51 %\n",
      "\n",
      "12:00 - 17:59 : 14.49 : ( 44936 ) : 47.39 %\n",
      "\n",
      "18:00 - 23:59 : 8.04 : ( 23403 ) : 24.68 %\n",
      "\n",
      "For Show HN\n",
      "4-Quarters of Day : Average Comments per Post : (Magnitude of Comments) : Percentage of Total Comments\n",
      "0:00 - 5:59 : 4.51 : ( 5865 ) : 11.82 %\n",
      "\n",
      "6:00 - 11:59 : 5.26 : ( 9301 ) : 18.74 %\n",
      "\n",
      "12:00 - 17:59 : 5.12 : ( 21580 ) : 43.49 %\n",
      "\n",
      "18:00 - 23:59 : 4.5 : ( 12874 ) : 25.95 %\n",
      "\n"
     ]
    }
   ],
   "source": [
    "day_quartered_freq = {}\n",
    "data_set_names = ['All HN', 'Ask HN', 'Show HN']\n",
    "list_count = 0\n",
    "\n",
    "for data_set in comments_hours_mins_container:\n",
    "    day_quartered_freq['0:00 - 5:59'] = [0, 0]\n",
    "    day_quartered_freq['6:00 - 11:59'] = [0, 0]\n",
    "    day_quartered_freq['12:00 - 17:59'] = [0, 0]\n",
    "    day_quartered_freq['18:00 - 23:59'] = [0, 0]\n",
    "    midnight = dt.datetime.strptime('0:00', \"%H:%M\")\n",
    "    six_a = dt.datetime.strptime('6:00', \"%H:%M\")\n",
    "    noon = dt.datetime.strptime('12:00', \"%H:%M\")\n",
    "    six_p = dt.datetime.strptime('18:00', \"%H:%M\")\n",
    "    total_comments = 0\n",
    "\n",
    "    for row in comments_hours_mins_container[list_count]:\n",
    "        comment = row[0]\n",
    "        hours_and_min = row[1]\n",
    "        hours_and_min = dt.datetime.strptime(hours_and_min, \"%H:%M\")   \n",
    "        if hours_and_min >= midnight and hours_and_min < six_a: \n",
    "            day_quartered_freq['0:00 - 5:59'][0] += comment\n",
    "            day_quartered_freq['0:00 - 5:59'][1] += 1\n",
    "        elif hours_and_min >= six_a and hours_and_min < noon:\n",
    "            day_quartered_freq['6:00 - 11:59'][0] += comment\n",
    "            day_quartered_freq['6:00 - 11:59'][1] += 1\n",
    "        elif hours_and_min >= noon and hours_and_min < six_p:\n",
    "            day_quartered_freq['12:00 - 17:59'][0] += comment\n",
    "            day_quartered_freq['12:00 - 17:59'][1] += 1\n",
    "        elif hours_and_min >= six_p:\n",
    "            day_quartered_freq['18:00 - 23:59'][0] += comment\n",
    "            day_quartered_freq['18:00 - 23:59'][1] += 1\n",
    "        total_comments += comment\n",
    "    \n",
    "    print(f\"For {data_set_names[list_count]}\")\n",
    "    print('4-Quarters of Day : Average Comments per Post : (Magnitude of Comments) : Percentage of Total Comments')\n",
    "    for row in day_quartered_freq:\n",
    "        print(row, ':', round(day_quartered_freq[row][0] / day_quartered_freq[row][1], 2), ': (', day_quartered_freq[row][0], ') :', round((100 * day_quartered_freq[row][0] / total_comments), 2), '%\\n')\n",
    "    list_count += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The second sweep adds more color, it looks like the '12:00 - 17:59' has the most interaction (comments and posts), but the average comments per post remain close for all but the 'Ask HN' posts. The '12:00 - 17:59' window may contain our optimal time but the time-windows are not tight enough to make that call. There may also be a shifted window that contains the most comments. With that in mind, we will break the data down into 24 windows, 1 hour each using the same script process as above, just for 24 time-windows this time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For All HN\n",
      "Post Creation Time : Average Comments per Post : (Magnitude of Comments) : Percentage of Total Comments\n",
      "0:00 - 0:59 : 6.58 : ( 59051 ) : 3.09 %\n",
      "\n",
      "1:00 - 1:59 : 6.42 : ( 50851 ) : 2.66 %\n",
      "\n",
      "2:00 - 2:59 : 7.27 : ( 54172 ) : 2.83 %\n",
      "\n",
      "3:00 - 3:59 : 6.43 : ( 45851 ) : 2.4 %\n",
      "\n",
      "4:00 - 4:59 : 6.63 : ( 47091 ) : 2.46 %\n",
      "\n",
      "5:00 - 5:59 : 6.76 : ( 44203 ) : 2.31 %\n",
      "\n",
      "6:00 - 6:59 : 6.17 : ( 45541 ) : 2.38 %\n",
      "\n",
      "7:00 - 7:59 : 6.1 : ( 47586 ) : 2.49 %\n",
      "\n",
      "8:00 - 8:59 : 6.34 : ( 53937 ) : 2.82 %\n",
      "\n",
      "9:00 - 9:59 : 6.52 : ( 59029 ) : 3.09 %\n",
      "\n",
      "10:00 - 10:59 : 6.51 : ( 63388 ) : 3.31 %\n",
      "\n",
      "11:00 - 11:59 : 7.37 : ( 76282 ) : 3.99 %\n",
      "\n",
      "12:00 - 12:59 : 7.69 : ( 97925 ) : 5.12 %\n",
      "\n",
      "13:00 - 13:59 : 7.34 : ( 116861 ) : 6.11 %\n",
      "\n",
      "14:00 - 14:59 : 6.46 : ( 117088 ) : 6.12 %\n",
      "\n",
      "15:00 - 15:59 : 7.05 : ( 137635 ) : 7.2 %\n",
      "\n",
      "16:00 - 16:59 : 6.18 : ( 124557 ) : 6.51 %\n",
      "\n",
      "17:00 - 17:59 : 1.0 : ( 127000 ) : 6.64 %\n",
      "\n",
      "18:00 - 18:59 : 6.46 : ( 120621 ) : 6.31 %\n",
      "\n",
      "19:00 - 19:59 : 6.33 : ( 107872 ) : 5.64 %\n",
      "\n",
      "20:00 - 20:59 : 5.95 : ( 94965 ) : 4.96 %\n",
      "\n",
      "21:00 - 21:59 : 5.94 : ( 86255 ) : 4.51 %\n",
      "\n",
      "22:00 - 22:59 : 5.87 : ( 72881 ) : 3.81 %\n",
      "\n",
      "23:00 - 23:59 : 5.98 : ( 62119 ) : 3.25 %\n",
      "\n",
      "For Ask HN\n",
      "Post Creation Time : Average Comments per Post : (Magnitude of Comments) : Percentage of Total Comments\n",
      "0:00 - 0:59 : 7.59 : ( 2261 ) : 2.38 %\n",
      "\n",
      "1:00 - 1:59 : 7.36 : ( 2068 ) : 2.18 %\n",
      "\n",
      "2:00 - 2:59 : 11.14 : ( 2996 ) : 3.16 %\n",
      "\n",
      "3:00 - 3:59 : 7.97 : ( 2153 ) : 2.27 %\n",
      "\n",
      "4:00 - 4:59 : 9.8 : ( 2353 ) : 2.48 %\n",
      "\n",
      "5:00 - 5:59 : 8.79 : ( 1838 ) : 1.94 %\n",
      "\n",
      "6:00 - 6:59 : 6.78 : ( 1587 ) : 1.67 %\n",
      "\n",
      "7:00 - 7:59 : 7.04 : ( 1584 ) : 1.67 %\n",
      "\n",
      "8:00 - 8:59 : 9.19 : ( 2362 ) : 2.49 %\n",
      "\n",
      "9:00 - 9:59 : 6.65 : ( 1477 ) : 1.56 %\n",
      "\n",
      "10:00 - 10:59 : 10.72 : ( 3011 ) : 3.18 %\n",
      "\n",
      "11:00 - 11:59 : 9.01 : ( 2794 ) : 2.95 %\n",
      "\n",
      "12:00 - 12:59 : 12.4 : ( 4228 ) : 4.46 %\n",
      "\n",
      "13:00 - 13:59 : 16.37 : ( 7219 ) : 7.61 %\n",
      "\n",
      "14:00 - 14:59 : 9.71 : ( 4970 ) : 5.24 %\n",
      "\n",
      "15:00 - 15:59 : 28.68 : ( 18525 ) : 19.54 %\n",
      "\n",
      "16:00 - 16:59 : 7.73 : ( 4458 ) : 4.7 %\n",
      "\n",
      "17:00 - 17:59 : 1.0 : ( 5536 ) : 5.84 %\n",
      "\n",
      "18:00 - 18:59 : 7.91 : ( 4824 ) : 5.09 %\n",
      "\n",
      "19:00 - 19:59 : 7.19 : ( 3949 ) : 4.16 %\n",
      "\n",
      "20:00 - 20:59 : 8.73 : ( 4463 ) : 4.71 %\n",
      "\n",
      "21:00 - 21:59 : 8.72 : ( 4500 ) : 4.75 %\n",
      "\n",
      "22:00 - 22:59 : 8.82 : ( 3369 ) : 3.55 %\n",
      "\n",
      "23:00 - 23:59 : 6.68 : ( 2298 ) : 2.42 %\n",
      "\n",
      "For Show HN\n",
      "Post Creation Time : Average Comments per Post : (Magnitude of Comments) : Percentage of Total Comments\n",
      "0:00 - 0:59 : 4.65 : ( 1283 ) : 2.59 %\n",
      "\n",
      "1:00 - 1:59 : 4.09 : ( 1003 ) : 2.02 %\n",
      "\n",
      "2:00 - 2:59 : 5.15 : ( 1076 ) : 2.17 %\n",
      "\n",
      "3:00 - 3:59 : 4.53 : ( 934 ) : 1.88 %\n",
      "\n",
      "4:00 - 4:59 : 5.04 : ( 978 ) : 1.97 %\n",
      "\n",
      "5:00 - 5:59 : 3.46 : ( 591 ) : 1.19 %\n",
      "\n",
      "6:00 - 6:59 : 4.71 : ( 904 ) : 1.82 %\n",
      "\n",
      "7:00 - 7:59 : 6.69 : ( 1573 ) : 3.17 %\n",
      "\n",
      "8:00 - 8:59 : 5.64 : ( 1770 ) : 3.57 %\n",
      "\n",
      "9:00 - 9:59 : 4.69 : ( 1411 ) : 2.84 %\n",
      "\n",
      "10:00 - 10:59 : 3.8 : ( 1228 ) : 2.47 %\n",
      "\n",
      "11:00 - 11:59 : 6.01 : ( 2415 ) : 4.87 %\n",
      "\n",
      "12:00 - 12:59 : 6.99 : ( 3609 ) : 7.27 %\n",
      "\n",
      "13:00 - 13:59 : 5.43 : ( 3314 ) : 6.68 %\n",
      "\n",
      "14:00 - 14:59 : 5.52 : ( 3839 ) : 7.74 %\n",
      "\n",
      "15:00 - 15:59 : 4.59 : ( 3822 ) : 7.7 %\n",
      "\n",
      "16:00 - 16:59 : 4.71 : ( 3768 ) : 7.59 %\n",
      "\n",
      "17:00 - 17:59 : 1.0 : ( 3228 ) : 6.51 %\n",
      "\n",
      "18:00 - 18:59 : 4.95 : ( 3249 ) : 6.55 %\n",
      "\n",
      "19:00 - 19:59 : 5.02 : ( 2791 ) : 5.62 %\n",
      "\n",
      "20:00 - 20:59 : 4.16 : ( 2182 ) : 4.4 %\n",
      "\n",
      "21:00 - 21:59 : 4.1 : ( 1759 ) : 3.54 %\n",
      "\n",
      "22:00 - 22:59 : 3.86 : ( 1450 ) : 2.92 %\n",
      "\n",
      "23:00 - 23:59 : 4.54 : ( 1443 ) : 2.91 %\n",
      "\n"
     ]
    }
   ],
   "source": [
    "hourly_freq = {}\n",
    "data_set_names = ['All HN', 'Ask HN', 'Show HN']\n",
    "list_count = 0\n",
    "\n",
    "for data_set in comments_hours_mins_container:\n",
    "    hourly_freq['0:00 - 0:59'] = [0, 0]\n",
    "    hourly_freq['1:00 - 1:59'] = [0, 0]\n",
    "    hourly_freq['2:00 - 2:59'] = [0, 0]\n",
    "    hourly_freq['3:00 - 3:59'] = [0, 0]\n",
    "    hourly_freq['4:00 - 4:59'] = [0, 0]\n",
    "    hourly_freq['5:00 - 5:59'] = [0, 0]\n",
    "    hourly_freq['6:00 - 6:59'] = [0, 0]\n",
    "    hourly_freq['7:00 - 7:59'] = [0, 0]\n",
    "    hourly_freq['8:00 - 8:59'] = [0, 0]\n",
    "    hourly_freq['9:00 - 9:59'] = [0, 0]\n",
    "    hourly_freq['10:00 - 10:59'] = [0, 0]\n",
    "    hourly_freq['11:00 - 11:59'] = [0, 0]\n",
    "    hourly_freq['12:00 - 12:59'] = [0, 0]\n",
    "    hourly_freq['13:00 - 13:59'] = [0, 0]\n",
    "    hourly_freq['14:00 - 14:59'] = [0, 0]\n",
    "    hourly_freq['15:00 - 15:59'] = [0, 0]\n",
    "    hourly_freq['16:00 - 16:59'] = [0, 0]\n",
    "    hourly_freq['17:00 - 17:59'] = [0, 0]\n",
    "    hourly_freq['18:00 - 18:59'] = [0, 0]\n",
    "    hourly_freq['19:00 - 19:59'] = [0, 0]\n",
    "    hourly_freq['20:00 - 20:59'] = [0, 0]\n",
    "    hourly_freq['21:00 - 21:59'] = [0, 0]\n",
    "    hourly_freq['22:00 - 22:59'] = [0, 0]\n",
    "    hourly_freq['23:00 - 23:59'] = [0, 0]\n",
    "    midnight = dt.datetime.strptime('0:00', \"%H:%M\")\n",
    "    increment_hour = dt.timedelta(hours=1)\n",
    "    total_comments = 0\n",
    "\n",
    "    for row in comments_hours_mins_container[list_count]:\n",
    "        comment = row[0]\n",
    "        hours_and_min = row[1]\n",
    "        hours_and_min = dt.datetime.strptime(hours_and_min, \"%H:%M\")   \n",
    "        if hours_and_min >= midnight and hours_and_min < (midnight + increment_hour): \n",
    "            hourly_freq['0:00 - 0:59'][0] += comment\n",
    "            hourly_freq['0:00 - 0:59'][1] += 1\n",
    "        elif hours_and_min >= (midnight + increment_hour) and hours_and_min < (midnight + 2 * increment_hour): \n",
    "            hourly_freq['1:00 - 1:59'][0] += comment\n",
    "            hourly_freq['1:00 - 1:59'][1] += 1\n",
    "        elif hours_and_min >= (midnight + 2 * increment_hour) and hours_and_min < (midnight + 3 * increment_hour): \n",
    "            hourly_freq['2:00 - 2:59'][0] += comment\n",
    "            hourly_freq['2:00 - 2:59'][1] += 1\n",
    "        elif hours_and_min >= (midnight + 3 * increment_hour) and hours_and_min < (midnight + 4 * increment_hour): \n",
    "            hourly_freq['3:00 - 3:59'][0] += comment\n",
    "            hourly_freq['3:00 - 3:59'][1] += 1\n",
    "        elif hours_and_min >= (midnight + 4 * increment_hour) and hours_and_min < (midnight + 5 * increment_hour): \n",
    "            hourly_freq['4:00 - 4:59'][0] += comment\n",
    "            hourly_freq['4:00 - 4:59'][1] += 1\n",
    "        elif hours_and_min >= (midnight + 5 * increment_hour) and hours_and_min < (midnight + 6 * increment_hour): \n",
    "            hourly_freq['5:00 - 5:59'][0] += comment\n",
    "            hourly_freq['5:00 - 5:59'][1] += 1\n",
    "        elif hours_and_min >= (midnight + 6 * increment_hour) and hours_and_min < (midnight + 7 * increment_hour): \n",
    "            hourly_freq['6:00 - 6:59'][0] += comment\n",
    "            hourly_freq['6:00 - 6:59'][1] += 1\n",
    "        elif hours_and_min >= (midnight + 7 * increment_hour) and hours_and_min < (midnight + 8 * increment_hour): \n",
    "            hourly_freq['7:00 - 7:59'][0] += comment\n",
    "            hourly_freq['7:00 - 7:59'][1] += 1\n",
    "        elif hours_and_min >= (midnight + 8 * increment_hour) and hours_and_min < (midnight + 9 * increment_hour): \n",
    "            hourly_freq['8:00 - 8:59'][0] += comment\n",
    "            hourly_freq['8:00 - 8:59'][1] += 1\n",
    "        elif hours_and_min >= (midnight + 9 * increment_hour) and hours_and_min < (midnight + 10 * increment_hour): \n",
    "            hourly_freq['9:00 - 9:59'][0] += comment\n",
    "            hourly_freq['9:00 - 9:59'][1] += 1\n",
    "        elif hours_and_min >= (midnight + 10 * increment_hour) and hours_and_min < (midnight + 11 * increment_hour): \n",
    "            hourly_freq['10:00 - 10:59'][0] += comment\n",
    "            hourly_freq['10:00 - 10:59'][1] += 1\n",
    "        elif hours_and_min >= (midnight + 11 * increment_hour) and hours_and_min < (midnight + 12 * increment_hour): \n",
    "            hourly_freq['11:00 - 11:59'][0] += comment\n",
    "            hourly_freq['11:00 - 11:59'][1] += 1\n",
    "        elif hours_and_min >= (midnight + 12 * increment_hour) and hours_and_min < (midnight + 13 * increment_hour): \n",
    "            hourly_freq['12:00 - 12:59'][0] += comment\n",
    "            hourly_freq['12:00 - 12:59'][1] += 1\n",
    "        elif hours_and_min >= (midnight + 13 * increment_hour) and hours_and_min < (midnight + 14 * increment_hour): \n",
    "            hourly_freq['13:00 - 13:59'][0] += comment\n",
    "            hourly_freq['13:00 - 13:59'][1] += 1\n",
    "        elif hours_and_min >= (midnight + 14 * increment_hour) and hours_and_min < (midnight + 15 * increment_hour): \n",
    "            hourly_freq['14:00 - 14:59'][0] += comment\n",
    "            hourly_freq['14:00 - 14:59'][1] += 1\n",
    "        elif hours_and_min >= (midnight + 15 * increment_hour) and hours_and_min < (midnight + 16 * increment_hour): \n",
    "            hourly_freq['15:00 - 15:59'][0] += comment\n",
    "            hourly_freq['15:00 - 15:59'][1] += 1\n",
    "        elif hours_and_min >= (midnight + 16 * increment_hour) and hours_and_min < (midnight + 17 * increment_hour): \n",
    "            hourly_freq['16:00 - 16:59'][0] += comment\n",
    "            hourly_freq['16:00 - 16:59'][1] += 1\n",
    "        elif hours_and_min >= (midnight + 17 * increment_hour) and hours_and_min < (midnight + 18 * increment_hour): \n",
    "            hourly_freq['17:00 - 17:59'][0] += comment\n",
    "            hourly_freq['17:00 - 17:59'][1] += comment\n",
    "        elif hours_and_min >= (midnight + 18 * increment_hour) and hours_and_min < (midnight + 19 * increment_hour): \n",
    "            hourly_freq['18:00 - 18:59'][0] += comment\n",
    "            hourly_freq['18:00 - 18:59'][1] += 1\n",
    "        elif hours_and_min >= (midnight + 19 * increment_hour) and hours_and_min < (midnight + 20 * increment_hour): \n",
    "            hourly_freq['19:00 - 19:59'][0] += comment\n",
    "            hourly_freq['19:00 - 19:59'][1] += 1\n",
    "        elif hours_and_min >= (midnight + 20 * increment_hour) and hours_and_min < (midnight + 21 * increment_hour): \n",
    "            hourly_freq['20:00 - 20:59'][0] += comment\n",
    "            hourly_freq['20:00 - 20:59'][1] += 1\n",
    "        elif hours_and_min >= (midnight + 21 * increment_hour) and hours_and_min < (midnight + 22 * increment_hour): \n",
    "            hourly_freq['21:00 - 21:59'][0] += comment\n",
    "            hourly_freq['21:00 - 21:59'][1] += 1\n",
    "        elif hours_and_min >= (midnight + 22 * increment_hour) and hours_and_min < (midnight + 23 * increment_hour): \n",
    "            hourly_freq['22:00 - 22:59'][0] += comment\n",
    "            hourly_freq['22:00 - 22:59'][1] += 1\n",
    "        elif hours_and_min >= (midnight + 23 * increment_hour): \n",
    "            hourly_freq['23:00 - 23:59'][0] += comment\n",
    "            hourly_freq['23:00 - 23:59'][1] += 1\n",
    "        total_comments += comment\n",
    "\n",
    "  \n",
    "    print(f\"For {data_set_names[list_count]}\")\n",
    "    print('Post Creation Time : Average Comments per Post : (Magnitude of Comments) : Percentage of Total Comments')\n",
    "    for row in hourly_freq:\n",
    "        print(row, ':', round(hourly_freq[row][0] / hourly_freq[row][1], 2), ': (', hourly_freq[row][0], ') :', round((100 * hourly_freq[row][0] / total_comments), 2), '%\\n')\n",
    "     \n",
    "    list_count += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "The number that sticks out is 28.68 posts per comments generated for the 15:00 - 15:59 time-slot of ther 'Ask HN' posts, the next highest is 16.37 at 13:00 - 13:59 for 'Ask HN' posts, 57.08% of the max. The highest posts per comment average in the Overall HN Data List is 7.69 at 12:00 - 12:59 and 6.99 at 12:00 - 12:59 for 'Show HN' Posts, 3.72 and 4.10 times less than the 28.68 max, respectively.\n",
    "\n",
    "But, why?\n",
    "\n",
    "The time zone for our timestamp data is Eastern Time (EST) in the US, which makes the rise in comment activity correlate well with working America's lunch break on the east coast. As lunch breaks occur in the time zones Westward, the magnitude of comments increases across the board, peaking when the West coast and tech hub of the US (thinking Silicon Valley in the year ranges of the data 2015 - 2016) has their lunch break. Y Combinator, start-up that created Hacker News is based in Mountain View, CA. It would make sense that the West Coast would contain a majority of users as the popularity of the social news site would diffuse from that central location. That combined with our initial hypothesis that Asking a question incites a higher response rate gives us our time and post-type for maximum comment return: 'Ask HN' at the 13:00 - 13:59 time slot."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
